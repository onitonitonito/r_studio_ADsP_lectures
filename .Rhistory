install.packages("MASS")
library(MASS)
head(ChikWeight)
data(ChickWeight, package="datasets")
head(ChickWeight)
Chick <- ChickWeight[ChickWeight$Diet == 1, ]
Chick
Chick <- ChickWeight[ChickWeight$Chick == 1, ]
Chick
m <- lm(weight~Time, data=Chick)
m
#  weight = 24.465 + 7.988 * time
summary(m)
# F통계량 : 232.9
# p-value : 0.0000002974 (유의수준 5%하에 회귀모형이 통계적으로 유의함)
# 회귀계수 : time 의 p-value가 0.05 보다 작아서 회귀계수의 추정이 통계적으로 유의함
# 결정계수 R-squared = 0.9588, Adjusted R-squared = 0.9547 : 회귀식이 데이터를 적절하게 설명함
# 예제: cars 데이터를 활용한 곡선회귀분석
data(cars, package="datasets")
cars
speed2 <- cars$speed^2
cars <- cbind(speed2, cars)
cars
m <- lm(dist ~ speed + speed2, data=cars)
m
#  dist = 2.47014 + 0.91329 *speed  +0.09996 * speed^2
summary(m)
# F통계량 : 47.14
# p-value : 5.852e-12 (유의수준 5%하에 회귀모형이 통계적으로 유의함)
# 회귀계수 : speed 의 p-value가 0.05 보다 커서 회귀계수의 추정이 통계적으로 유의하지 못함
# 결정계수 R-squared = 0.6673, Adjusted R-squared = 0.6532 : 회귀식이 데이터를 적절하게 설명한다.
#2차 곡선 회귀분석
x <- c(1:9)
y <- c(5, 3, 2, 3, 4, 6, 10, 12, 18)
df1 <- data.frame(x, y)
df1
plot(df1)
x2 <- x^2
df2 <- cbind(x2, df1)
df2
m <- lm(y ~ x, data=df1)
m
#  y = -1.167 + 1.633 *x
summary(m)
# F통계량 : 16.99
# p-value : 0.0044 (유의수준 5%하에 회귀모형이 통계적으로 유의함)
# 회귀계수 : speed 의 p-value가 0.05 보다 작아 회귀계수의 추정이 통계적으로 유의함
# 결정계수 R-squared = 0.7083, Adjusted R-squared = 0.666 : 회귀식이 데이터를 적절하게 설명한다.
m2 <- lm(y ~ x + x2, data=df2)
m2
#  y = 7.1667 + -2.9121 *x +0.4545 *x^2
summary(m2)
# F통계량 : 292.2
# p-value : 1.05e-06 (유의수준 5%하에 회귀모형이 통계적으로 유의함)
# 회귀계수 : speed 의 p-value가 0.05 보다 작아 회귀계수의 추정이 통계적으로 유의함
# 결정계수 R-squared = 0.9898, Adjusted R-squared = 0.9864 : 회귀식이 데이터를 적절하게 설명한다.
###-----------------------------------------------------------------------------
###  logistic regression analysis
###-----------------------------------------------------------------------------
library(boot)
data(nodal)
nodal
rd <- nodal[,-1]
gfit <- glm(r~., data=rd, family="binomial")
summary(gfit)
# 최종 회귀식
a<-c(2,4,6,7)
rd<-nodal[,a]
rd
gfit <- glm(r~., data=rd, family="binomial")
summary(gfit)
###-----------------------------------------------------------------------------
###  regression analysis  (variable selection)
###-----------------------------------------------------------------------------
#Backward selection
x1 <- c(7, 1, 11, 11, 7, 11, 3, 1, 2,21, 1,11, 10)
x2 <- c(26, 29, 56, 31, 52, 55, 71,31, 54, 47, 40, 66, 68)
x3 <- c(6, 15, 8, 8, 6, 9, 17, 22, 18, 4, 23, 9, 8)
x4 <- c(60, 52, 20, 47, 33, 22, 6, 44, 22, 26, 34, 12, 12)
y <- c(78.5, 74.3, 104.3, 87.6, 95.9, 109.2, 102.7, 72.5, 93.1, 115.9, 83.8, 113.3, 109.4)
df <- data.frame(x1, x2, x3, x4, y)
df
plot(df)
a <- lm(y ~ x1 + x2 + x3 + x4, data=df)
summary(a)
b <- lm(y ~ x1 + x2 + x4, data=df)
summary(b)
c <- lm(y ~ x1 + x2, data=df)
summary(c)
#--- direction : forward, backward, both
step(lm(y ~ 1, data=df), scope=list(lower = ~ 1, upper = ~ x1 + x2 + x3 + x4), direction = "forward")
step(lm(y ~ x1 + x2 + x3 + x4, data=df), scope=list(lower = ~ 1, upper = ~ x1 + x2 + x3 + x4), direction = "backward")
step(lm(y ~ 1, data=df), scope=list(lower = ~ 1, upper = ~ x1 + x2 + x3 + x4), direction = "both")
#MASS 패키지의 hills 데이터를 활용해서 time을 종속변수로 전진선택법으로 회귀분석
data(hills, package="MASS")
head(hills)
step(lm(time ~ 1, data=hills), scope=list(lower = ~ 1, upper = ~ dist + climb + time), direction = "forward")
# 데이터 셋을 불러들여 setp 함수를 통한 회귀분석 실시
#data <- read.table("E:/ADSP/교육자료/4.2/data/4.2_003.csv", header=TRUE, sep=",",
stringsAsFactors=FALSE, na.strings=c('NIL'),
comment.char="#", encoding="UTF-8")
Bio <- read.csv("E:/ADSP/교육자료/4.2/data/4.2_003.csv", header=T, encoding="UTF-8")
head(Bio)
step(lm(Pemax ~ 1, data=Bio), scope=list(lower = ~ 1, upper = ~ age + height + weight + BMP + RV + FRC + TLC), direction = "forward")
step(lm(Pemax ~ age + height + weight + BMP + RV + FRC + TLC, data=Bio), scope=list(lower = ~ 1, upper = ~ age + height + weight + BMP + RV + FRC + TLC), direction = "backward")
step(lm(Pemax ~ 1, data=Bio), scope=list(lower = ~ 1, upper = ~ age + height + weight + BMP + RV + FRC + TLC), direction = "both")
###-----------------------------------------------------------------------------
### chpt 3. time series
###-----------------------------------------------------------------------------
install.packages("TTR")
library(TTR)
install.packages("forecast")
install.packages("MASS")
ggplot2
library(ggplot2)
ggplot2
a <- 42
A <- a * 2        # R is case sensitive
print(a)          # print function is limited in many way, so use cat()
cat(A, "\n")      # "84" is concatenated with "\n"
if(A > a)         # true, 84 > 42
{
cat(A, ">", a, "\n")
}
rm(list=ls())
setwd("~/Github/r_studio_ADsP_lectures/book")
getwd()
setwd("~/Github/r_studio_ADsP_lectures")
getwd()
###-----------------------------------------------------------------------------
### 1.descriptive statistics
###-----------------------------------------------------------------------------
data(iris)
length(iris$Species)
colnames(iris)
head(iris)
summary(iris)
plot(iris)
se_L <- iris$Sepal.Length
mean(se_L)
median(se_L)
sd(se_L)
var(se_L)
hist(se_L)
quantile(se_L)
quantile(se_L, 3/4)
max(se_L)
min(se_L)
y <- iris[iris$Species == 'setosa',]$Petal.Length
x <- iris[iris$Species == 'setosa',]$Petal.Width
y1 <- iris$Petal.Length
x1 <- iris$Petal.Width
hist(x)
hist(y)
plot(y, x)
hist(x1)
hist(y1)
plot(y1, x1)
m1 = lm(formula=y1~x1, data=iris)
abline(m1, lty="dotted")
summary(m1)
rm(list=ls())
data(iris)
length(iris$Species)
colnames(iris)
head(iris)
summary(iris)
boxplot(iris)
boxplot(iris$Sepal.Length, iris$Petal.Length, iris$Sepal.Width, iris$Petal.Width)
ggplot(data=iris, aes(x=iris$Sepal.Length, iris$Petal.Length, iris$Sepal.Width, iris$Petal.Width,y=iris$Species))
ggplot(data=iris, aes(x=(iris$Sepal.Length, iris$Petal.Length, iris$Sepal.Width, iris$Petal.Width),y=iris$Species))
ggplot(data=iris, aes(x=(`iris$Sepal.Length`, `iris$Petal.Length`, `iris$Sepal.Width`, `iris$Petal.Width`),y=iris$Species))
ggplot(data=iris, aes(x=iris$Sepal.Length,y=iris$Species))
ggplot(data=iris, aes(x=iris$Sepal.Length,y=iris$Species)) + geom_point()
ggplot(data=iris, aes(x=iris$Sepal.Width,y=iris$Species)) + geom_point()
ggplot(data=iris, aes(x=iris$Sepal.Width,y=iris$Sepal.Length)) + geom_point()
ggplot(data=iris, aes(x=iris$Petal.Width,y=iris$Petal.Length)) + geom_point()
rm(list=ls())
setwd("~/Github/r_studio_ADsP_lectures/book")
getwd()
setwd("~/Github/r_studio_ADsP_lectures")
getwd()
###-----------------------------------------------------------------------------
### 1.descriptive statistics
###-----------------------------------------------------------------------------
data(iris)
length(iris$Species)
colnames(iris)
head(iris)
summary(iris)
plot(iris)
ggplot(data=iris, aes(x=iris$Sepal.Width,y=iris$Sepal.Length)) + geom_point()
ggplot(data=iris, aes(x=iris$Petal.Width,y=iris$Petal.Length)) + geom_point()
se_L <- iris$Sepal.Length
mean(se_L)
median(se_L)
sd(se_L)
var(se_L)
hist(se_L)
quantile(se_L)
quantile(se_L, 3/4)
max(se_L)
min(se_L)
y <- iris[iris$Species == 'setosa',]$Petal.Length
x <- iris[iris$Species == 'setosa',]$Petal.Width
y1 <- iris$Petal.Length
x1 <- iris$Petal.Width
hist(x)
hist(y)
plot(y, x)
hist(x1)
hist(y1)
plot(y1, x1)
m1 = lm(formula=y1~x1, data=iris)
abline(m1, lty="dotted")
summary(m1)
rm(list=ls())
setwd("~/Github/r_studio_ADsP_lectures/book")
getwd()
setwd("~/Github/r_studio_ADsP_lectures")
getwd()
###-----------------------------------------------------------------------------
### 1.descriptive statistics
###-----------------------------------------------------------------------------
data(iris)
length(iris$Species)
colnames(iris)
head(iris)
summary(iris)
plot(iris)
ggplot(data=iris, aes(x=iris$Sepal.Width,y=iris$Sepal.Length)) + geom_point()
ggplot(data=iris, aes(x=iris$Petal.Width,y=iris$Petal.Length)) + geom_point()
se_L <- iris$Sepal.Length
mean(se_L)
median(se_L)
sd(se_L)
var(se_L)
hist(se_L)
quantile(se_L)
quantile(se_L, 3/4)
max(se_L)
min(se_L)
y <- iris[iris$Species == 'setosa',]$Petal.Length
x <- iris[iris$Species == 'setosa',]$Petal.Width
y1 <- iris$Petal.Length
x1 <- iris$Petal.Width
hist(x)
hist(y)
plot(y, x)
hist(x1)
hist(y1)
plot(y1, x1)
m1 = lm(formula=y1~x1, data=iris)
abline(m1, lty="dotted")
summary(m1)
rm(list=ls())
getwd()
#############################################
#
#   계량적  MDS(다차원척도법)
#
#############################################
library(MASS)
" __ (1) 도시간의 거리를 알면, 도시 전체의 배치를 알수 있음 __"
data("eurodist")
eurodist
head(eurodist)
summary(eurodist)
loc<-cmdscale(eurodist)
x<-loc[,1]
y<- -loc[,2]
# 플롯 옵션
# 각 도시간의 거리를 알면, 실제 도시의 위치와 비슷하게 배치가 된다.
plot(x, y, type="n", asp=1, main="Metric MDS")
text(x,y,rownames(loc),cex=0.9)
abline(v=0, h=0, lty=2, lwd=0.5)
"__ (2) 서로 성격이 비슷한 차량끼리, 2차원 공간에 배치시켜 줌 ___"
data(mtcars)
head(mtcars)
mt <- mtcars
mt.x <- as.matrix(mt[,-1])
mt.dist <- dist(mt.x)
loc <- cmdscale(mt.dist)
# 그림상에 보이지 않지만, 점을 찍어주고
plot(loc[,1],loc[,2],type="n",asp=1,main="Metric MDS")
# 타이틀을 표시해 준다
text(loc[,1],loc[,2],rownames(loc),cex=0.7)
# 중앙값의 라인을 그려준다.
abline(v=0, h=0, lty=2, lwd=0.5)
#############################################
#
#   주성분분석 (PCA: Principal Component Analysis)
#
#############################################
library(datasets)
data(USArrests)
pairs(USArrests, panel=panel.smooth,
main="USArrests data")
US.prin <- princomp(USArrests,cor=TRUE)
summary(US.prin)
screeplot(US.prin,npcs=5, type="lines")
loadings(US.prin)
"
Loadings:
Comp.1  Comp.2    Comp.3 Comp.4
Murder   -0.536*  0.418    -0.341  0.649
Assault  -0.583*  0.188    -0.268 -0.743
UrbanPop -0.278  -0.873*   -0.378  0.134
Rape     -0.543* -0.167     0.818
__컴포넌트 팩터가 큰값 비교: PC1=M,A,R / PC2=U
"
US.prin$scores
biplot(US.prin)
"직선방향= 성향강함 / 반대쪽=성향약함 / 법선=영향없음"
#############################################
#
#   ???????? PCA ??�� 2
#
#############################################
# csv ???? ?ҷ??��? (file importing)
f_2007 <- read.csv("./_static\\finance_2007.csv",
header = TRUE, stringsAsFactors = FALSE)
head(f_2007)
# 6개 증권사현황 = 교보,메리츠,대신,대우,동부,SK
# V1 : 총자본순이익율
# V2 : 자기자본순이익율
# V3 : 자기자본비율
# V4 : 부채비율
# V5 : 자기자본회전율
# 표준화 변환 (standardization)
f_2007 <- transform(f_2007,
V1_s = scale(V1),
V2_s = scale(V2),
V3_s = scale(V3),
V4_s = scale(V4),
V5_s = scale(V5))
# variable selection
f_2007_2 <-f_2007[,c("company", "V1_s", "V2_s", "V3_s", "V4_s", "V5_s")]
head(f_2007_2)
# 상관관계(Correlation)분석
cor(f_2007_2[,-1])
round(cor(f_2007_2[,-1]), digits=3) # ?ݿø?
# Scatter plot matrix
# 스캐터 챠드 확인
plot(f_2007_2[,-1])
# 주성분분석 PCA(Principal Component Analysis)
secu_prcomp <- prcomp(f_2007_2[,c(2:6)]) # 첫번째 회사명은 제외(2~6컬럼)
summary(secu_prcomp)
print(secu_prcomp)
# Scree Plot
plot(secu_prcomp, type="l",
sub = "Scree Plot")
# Biplot
biplot(secu_prcomp, cex = c(0.7, 0.8))
# 관측치별 주성분1, 주성분2 점수 계산(PC1 score, PC2 score)
secu_pc1 <- predict(secu_prcomp)[,1]
secu_pc2 <- predict(secu_prcomp)[,2]
# 그래프상에 관측치별 이름 매핑(rownames mapping)
text(secu_pc1, secu_pc2, labels = f_2007_2$company,
cex = 0.7, pos = 3, col = "blue")
###########################################################
## PCA (Principal Component Analysis)
## User Defined Function
##  - finding PC k which Cummulative Proportion is over 0.8
###########################################################
pca <- function(dataset)  {
pc = prcomp(dataset, scale = TRUE)
k = 0
R = 0
while(R < 0.8) {
k = k + 1
R = sum(pc[[1]][1:k]^2)/sum(pc[[1]]^2)
cat("When number of Principal Component(k) is ", k,
", Cummulative Proportion(R) is ", R, "\n", "\n", sep="")
}
SelectedDataSet = pc[[5]][,1:k]
return(SelectedDataSet)
}
# pca 펑션을 정의해서, 각 데이터별 XY의 거리 데이터를 보여줌.
pca(f_2007_2[,c(2:6)])
## PCA for College data
data(College)
cdat = College[,2:18]
dim(cdat)
names(cdat)
## PCA
pc.col <- princomp(cdat) #default - centers and scales
#default R plots with princomp
biplot(pc.col, cex=.7)
screeplot(pc.col)
# #scatter plots - patterns among observations
i = 1; j = 2;
plot(pc.col$scores[,i],pc.col$scores[,j],pch=16,cex=.2)
text(pc.col$scores[,i],pc.col$scores[,j],rownames(cdat),cex=.6)
#look at a particular college
ind = match("Harvard University",rownames(cdat))
text(pc.col$scores[ind,i],pc.col$scores[ind,j],rownames(cdat)[ind],cex=.7,col=2)
#loadings - variables that contribute to these patterns
par(mfrow=c(2,1))
barplot(pc.col$loadings[,1],cex.names=.6,main="PC 1 Loadings")
barplot(pc.col$loadings[,2],cex.names=.6,main="PC 2 Loadings")
#variance explained
screeplot(pc.col)
varex <- 100*pc.col$sdev^2/sum(pc.col$sdev^2)
plot(varex,type="l",ylab="% Variance Explained",xlab="Component")
#cumulative variance explained
cvarex = NULL
for(i in 1:ncol(cdat)){
cvarex[i] = sum(varex[1:i])
}
plot(cvarex,type="l",ylab="Cumulative Variance Explained",xlab="Component")
library(College)
install.packages(College)
init <- function()
{
rm(list=ls())
setwd("~/Github/r_studio_ADsP_lectures")
getwd()
}
###-----------------------------------------------------------------------------
### 1.descriptive statistics
###-----------------------------------------------------------------------------
init()
?iris
data(iris)
head(iris)
summary(iris)
Se_L <- iris$Sepal.Length
mean(Se_L)
median(Se_L)
sd(Se_L)
var(Se_L)
quantile(Se_L)
quantile(Se_L, 3/4)
max(Se_L)
min(Se_L)
install.packages("MASS")
library(MASS)
data(Animals, package="MASS")
Animals
summary(Cars93)
An_B <- Animals$body
mean(An_B)
median(An_B)
sd(An_B)
var(An_B)
quantile(An_B, 1/4)
quantile(An_B, 3/4)
max(An_B)
min(An_B)
quantile(An_B)
An_b <- Animals$brain
mean(An_b)
median(An_b)
sd(An_b)
var(An_b)
quantile(An_b, 1/4)
quantile(An_b, 3/4)
max(An_b)
min(An_b)
quantile(An_b)
###-----------------------------------------------------------------------------
### 3. correlation analysis
###-----------------------------------------------------------------------------
init()
1+1
14-10
4*6
28/7
38%%7
oddcount <- function(x) {
k <- 0
for(n in x) {
if(n%%2==1) k <- k + 1
}
return(k)
}
oddcount(c(1,3,4))
oddcount(c(1,2,3,4,5,6,7))
data(mtcars)
plot(mtcars)
colnames(mtcars)
head(mtcars)
summary(mtcars)
a <- mtcars$mpg
b <- mtcars$hp
c<-mtcars$wt
plot(a, b)
plot(a, c)
cov(a,c)
cor(a,c)
cor(mtcars)
cov(mtcars)
install.packages("Hmisc")
install.packages("MASS")
x <- c(110,120,130,140,150)
y <- c(100,105,128,115,142)
plot(y~x)
plot(y~x, pch=20, col="red")
line = lm(y~x)
line
"""
Call:
lm(formula = y ~ x)
Coefficients:
(Intercept)            x
-4.20         0.94
"""
```
Call:
lm(formula = y ~ x)
abline(line, col="blue")
